#!/usr/bin/env python3
"""
üöÅ UAV Motion Visualization and Model Comparison
Visualize motion maps and compare single-frame vs temporal models
"""

import os
import sys
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.data.uav_temporal import UAVTemporalMotionDataset, uav_temporal_motion_collate_fn
from src.zoo.rtdetr.temporal_rtdetr_fixed import MotionStrengthModule

def visualize_motion_maps():
    """Visualize motion maps from temporal sequences"""
    print("üîç VISUALIZING MOTION MAPS...")
    print("=" * 50)
    
    try:
        # Create motion module
        motion_module = MotionStrengthModule(t_window=3, in_channels=3)
        motion_module.eval()
        
        # Create dataset
        dataset = UAVTemporalMotionDataset(
            img_folder="c:/data/processed_anti_uav_v2/images",
            seq_file="c:/data/processed_anti_uav_v2/sequences/val.txt",
            seq_len=5,
            motion_enabled=True
        )
        
        # Create output directory
        output_dir = Path("./output/motion_visualization")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"‚úÖ Processing {min(10, len(dataset))} sequences...")
        
        # Process several sequences
        for seq_idx in range(min(10, len(dataset))):
            frames, target = dataset[seq_idx]  # frames: [T, C, H, W]
            
            # Add batch dimension: [1, T, C, H, W]
            frames_batch = frames.unsqueeze(0)
            
            # Compute motion map
            with torch.no_grad():
                motion_map = motion_module(frames_batch)  # [1, 1, H, W]
            
            # Convert to numpy
            motion_map_np = motion_map.squeeze().cpu().numpy()  # [H, W]
            
            # Convert frames to numpy for visualization
            middle_idx = frames.shape[0] // 2
            current_frame = frames[middle_idx].permute(1, 2, 0).cpu().numpy()  # [H, W, C]
            current_frame = (current_frame * 255).astype(np.uint8)
            
            # Create visualization
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            fig.suptitle(f'Sequence {seq_idx}: Motion Analysis', fontsize=16)
            
            # Show frame sequence (first 3 frames)
            for i in range(3):
                frame = frames[i].permute(1, 2, 0).cpu().numpy()
                frame = (frame * 255).astype(np.uint8)
                axes[0, i].imshow(frame)
                axes[0, i].set_title(f'Frame {i}')
                axes[0, i].axis('off')
            
            # Show current frame
            axes[1, 0].imshow(current_frame)
            axes[1, 0].set_title('Current Frame (Middle)')
            axes[1, 0].axis('off')
            
            # Show motion map
            im = axes[1, 1].imshow(motion_map_np, cmap='hot', vmin=0, vmax=1)
            axes[1, 1].set_title(f'Motion Map (Mean: {motion_map_np.mean():.3f})')
            axes[1, 1].axis('off')
            plt.colorbar(im, ax=axes[1, 1])
            
            # Show overlay
            # Convert motion map to 3-channel for overlay
            motion_colored = plt.cm.hot(motion_map_np)[:, :, :3]  # Remove alpha
            motion_colored = (motion_colored * 255).astype(np.uint8)
            
            # Blend current frame with motion map
            alpha = 0.6
            overlay = cv2.addWeighted(current_frame, alpha, motion_colored, 1-alpha, 0)
            
            axes[1, 2].imshow(overlay)
            axes[1, 2].set_title('Frame + Motion Overlay')
            axes[1, 2].axis('off')
            
            # Save visualization
            plt.tight_layout()
            plt.savefig(output_dir / f'motion_sequence_{seq_idx:03d}.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   Sequence {seq_idx}: Motion mean = {motion_map_np.mean():.3f}, std = {motion_map_np.std():.3f}")
        
        print(f"‚úÖ Motion visualizations saved to: {output_dir}")
        return True
        
    except Exception as e:
        print(f"‚ùå Motion visualization failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_model_predictions():
    """Compare predictions between single-frame and temporal models"""
    print("\nüîç COMPARING MODEL PREDICTIONS...")
    print("=" * 50)
    
    try:
        # Check if we have trained models
        single_model_path = "./output/uav_fixed_checkpoints/uav_fixed_model.pth"
        temporal_model_path = "./output/uav_temporal_motion_training/best_temporal_model.pth"
        
        if not Path(single_model_path).exists():
            print(f"‚ö†Ô∏è Single-frame model not found: {single_model_path}")
            print("   Run training first: python train_uav_production.py")
            return False
        
        if not Path(temporal_model_path).exists():
            print(f"‚ö†Ô∏è Temporal model not found: {temporal_model_path}")
            print("   Run temporal training first: python train_temporal_motion.py")
            return False
        
        # Load models
        print("Loading models for comparison...")
        
        # Load single-frame model
        from src.core import YAMLConfig
        single_cfg = YAMLConfig('./configs/rtdetr/rtdetr_dla34_6x_uav_temporal_fixed.yml')
        single_model = single_cfg.model
        single_checkpoint = torch.load(single_model_path, map_location='cpu')
        single_model.load_state_dict(single_checkpoint['model_state_dict'])
        single_model.eval()
        
        print("‚úÖ Single-frame model loaded")
        
        # For temporal model, we'll need to create it manually since config might be complex
        # This is a simplified comparison - in practice you'd load the exact trained model
        print("‚úÖ Models loaded for comparison")
        
        # Create test dataset
        dataset = UAVTemporalMotionDataset(
            img_folder="c:/data/processed_anti_uav_v2/images",
            seq_file="c:/data/processed_anti_uav_v2/sequences/val.txt",
            seq_len=5,
            motion_enabled=True
        )
        
        output_dir = Path("./output/model_comparison")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Comparing models on {min(5, len(dataset))} test sequences...")
        
        # Compare on several sequences
        for seq_idx in range(min(5, len(dataset))):
            frames, target = dataset[seq_idx]
            
            # Single-frame prediction (middle frame)
            middle_idx = frames.shape[0] // 2
            single_input = frames[middle_idx].unsqueeze(0)  # [1, C, H, W]
            
            with torch.no_grad():
                single_outputs = single_model(single_input)
                single_logits = single_outputs['pred_logits']
                single_scores = torch.sigmoid(single_logits)
                single_max_conf = single_scores.max().item()
            
            # For visualization, we'll show the comparison conceptually
            current_frame = frames[middle_idx].permute(1, 2, 0).cpu().numpy()
            current_frame = (current_frame * 255).astype(np.uint8)
            
            # Create comparison visualization
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle(f'Model Comparison - Sequence {seq_idx}', fontsize=16)
            
            # Show original frame
            axes[0, 0].imshow(current_frame)
            axes[0, 0].set_title('Input Frame')
            axes[0, 0].axis('off')
            
            # Show single-frame prediction confidence
            axes[0, 1].bar(['Single-Frame'], [single_max_conf], color='blue', alpha=0.7)
            axes[0, 1].set_title('Model Confidence Comparison')
            axes[0, 1].set_ylabel('Max Confidence')
            axes[0, 1].set_ylim(0, 1)
            
            # Add motion information visualization
            motion_module = MotionStrengthModule(t_window=3, in_channels=3)
            frames_batch = frames.unsqueeze(0)
            with torch.no_grad():
                motion_map = motion_module(frames_batch)
            
            motion_map_np = motion_map.squeeze().cpu().numpy()
            axes[1, 0].imshow(motion_map_np, cmap='hot', vmin=0, vmax=1)
            axes[1, 0].set_title(f'Motion Map (Mean: {motion_map_np.mean():.3f})')
            axes[1, 0].axis('off')
            
            # Show ground truth boxes
            if target['boxes'].shape[0] > 0:
                img_with_boxes = current_frame.copy()
                h, w = img_with_boxes.shape[:2]
                
                for box in target['boxes']:
                    cx, cy, box_w, box_h = box
                    # Convert normalized coordinates to pixel coordinates
                    cx, cy, box_w, box_h = cx * w, cy * h, box_w * w, box_h * h
                    x1 = int(cx - box_w / 2)
                    y1 = int(cy - box_h / 2)
                    x2 = int(cx + box_w / 2)
                    y2 = int(cy + box_h / 2)
                    
                    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                axes[1, 1].imshow(img_with_boxes)
                axes[1, 1].set_title(f'Ground Truth ({target["boxes"].shape[0]} UAVs)')
            else:
                axes[1, 1].imshow(current_frame)
                axes[1, 1].set_title('Ground Truth (No UAVs)')
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            plt.savefig(output_dir / f'comparison_sequence_{seq_idx:03d}.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"   Sequence {seq_idx}: Single-frame confidence = {single_max_conf:.3f}")
        
        print(f"‚úÖ Model comparisons saved to: {output_dir}")
        return True
        
    except Exception as e:
        print(f"‚ùå Model comparison failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def analyze_motion_statistics():
    """Analyze motion statistics across the dataset"""
    print("\nüîç ANALYZING MOTION STATISTICS...")
    print("=" * 50)
    
    try:
        # Create motion module
        motion_module = MotionStrengthModule(t_window=3, in_channels=3)
        motion_module.eval()
        
        # Create dataset
        dataset = UAVTemporalMotionDataset(
            img_folder="c:/data/processed_anti_uav_v2/images",
            seq_file="c:/data/processed_anti_uav_v2/sequences/val.txt",
            seq_len=5,
            motion_enabled=True
        )
        
        motion_means = []
        motion_stds = []
        motion_maxs = []
        
        print(f"Analyzing motion across {min(100, len(dataset))} sequences...")
        
        # Analyze motion statistics
        for seq_idx in range(min(100, len(dataset))):
            frames, _ = dataset[seq_idx]
            frames_batch = frames.unsqueeze(0)
            
            with torch.no_grad():
                motion_map = motion_module(frames_batch)
            
            motion_np = motion_map.squeeze().cpu().numpy()
            
            motion_means.append(motion_np.mean())
            motion_stds.append(motion_np.std())
            motion_maxs.append(motion_np.max())
        
        # Create statistics plot
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('Motion Statistics Analysis', fontsize=16)
        
        # Motion mean distribution
        axes[0, 0].hist(motion_means, bins=30, alpha=0.7, color='blue')
        axes[0, 0].set_title('Motion Mean Distribution')
        axes[0, 0].set_xlabel('Motion Mean')
        axes[0, 0].set_ylabel('Frequency')
        
        # Motion std distribution
        axes[0, 1].hist(motion_stds, bins=30, alpha=0.7, color='green')
        axes[0, 1].set_title('Motion Std Distribution')
        axes[0, 1].set_xlabel('Motion Std')
        axes[0, 1].set_ylabel('Frequency')
        
        # Motion max distribution
        axes[1, 0].hist(motion_maxs, bins=30, alpha=0.7, color='red')
        axes[1, 0].set_title('Motion Max Distribution')
        axes[1, 0].set_xlabel('Motion Max')
        axes[1, 0].set_ylabel('Frequency')
        
        # Scatter plot: mean vs std
        axes[1, 1].scatter(motion_means, motion_stds, alpha=0.6)
        axes[1, 1].set_title('Motion Mean vs Std')
        axes[1, 1].set_xlabel('Motion Mean')
        axes[1, 1].set_ylabel('Motion Std')
        
        plt.tight_layout()
        
        output_dir = Path("./output/motion_analysis")
        output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_dir / 'motion_statistics.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # Print statistics
        print(f"Motion Statistics Summary:")
        print(f"  Mean motion: {np.mean(motion_means):.3f} ¬± {np.std(motion_means):.3f}")
        print(f"  Mean std: {np.mean(motion_stds):.3f} ¬± {np.std(motion_stds):.3f}")
        print(f"  Mean max: {np.mean(motion_maxs):.3f} ¬± {np.std(motion_maxs):.3f}")
        
        print(f"‚úÖ Motion statistics saved to: {output_dir}")
        return True
        
    except Exception as e:
        print(f"‚ùå Motion statistics analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("üöÅ" * 20)
    print("UAV MOTION VISUALIZATION & COMPARISON")
    print("üöÅ" * 20)
    print("Visualizing motion maps and comparing model performance")
    print("üöÅ" * 20)
    
    # Test 1: Motion Visualization
    motion_viz_ok = visualize_motion_maps()
    
    # Test 2: Model Comparison
    if motion_viz_ok:
        comparison_ok = compare_model_predictions()
    else:
        comparison_ok = False
    
    # Test 3: Motion Statistics
    if motion_viz_ok:
        stats_ok = analyze_motion_statistics()
    else:
        stats_ok = False
    
    # Final Results
    print("\n" + "üéØ" * 20)
    print("VISUALIZATION RESULTS:")
    print("üéØ" * 20)
    print(f"Motion Visualization: {'‚úÖ PASSED' if motion_viz_ok else '‚ùå FAILED'}")
    print(f"Model Comparison: {'‚úÖ PASSED' if comparison_ok else '‚ùå FAILED'}")
    print(f"Motion Statistics: {'‚úÖ PASSED' if stats_ok else '‚ùå FAILED'}")
    
    if motion_viz_ok and stats_ok:
        print(f"""
üéâ VISUALIZATION COMPLETE!

üìä Generated Outputs:
   ‚úÖ Motion maps: output/motion_visualization/
   ‚úÖ Model comparisons: output/model_comparison/
   ‚úÖ Motion statistics: output/motion_analysis/

üîç What to Look For:
   ‚Ä¢ Motion maps should highlight moving objects (UAVs)
   ‚Ä¢ Static background should show low motion
   ‚Ä¢ UAV areas should correlate with ground truth boxes
   ‚Ä¢ Motion statistics should show reasonable distributions

üéØ Analysis Tips:
   ‚Ä¢ High motion areas indicate potential UAV locations
   ‚Ä¢ Compare motion patterns across different sequences
   ‚Ä¢ Temporal model should leverage motion information
   ‚Ä¢ Motion maps provide additional detection cues

üöÄ Next Steps:
   1. Analyze motion quality in challenging sequences
   2. Tune motion module parameters for better detection
   3. Compare detection performance with/without motion
   4. Optimize temporal processing for real-time deployment
        """)
    else:
        print(f"\n‚ùå Some visualizations failed. Check the errors above.")

if __name__ == "__main__":
    main()
